[TOC]

# 调研报告

## 1 小组成员

* 田佳林 PB20061251
* 高弋超 PB20051083
* 朱仕哲 PB20111718
* 胡乐翔 PB19000165

## 2 项目简介

本项目计划基于 ROS 2 搭建一个分布式系统，并在此基础上建立一个利用现有的计算框架 Ray 对各个节点的算力进行分析和整合，从而实现节点间算力的调控和分配，提高分布式系统的性能和稳定性。

## 3 分布式计算

### 3.1 分布式系统简介

分布式计算是研究分布式系统的计算机科学研究领域。分布式系统(Distributed System)这个概念是相对于集中式系统来说的，简单而言，分布式系统指的是一组计算设备，这些计算机在物理上是分开的，在体系结构上可能是异构的，但是通过网络相互连接，形成一个统一的计算系统。分布式系统的各个计算机通过相互发送消息实现交流和协调。由于用户不能看到分布式系统的底层实现，所以分布式系统的各个计算设备对于用户而言是一个整体。

<img src="img/1.png">

### 3.2 分布式系统出现的背景

随着计算机网络的发展，程序需要用到的数据可能不再存在于单个的计算机上，例如，客户使用信用卡时，本地的机器可能只负责将数据发送到银行的计算机上，后者则进行验证和处理数据等工作。分布式系统所需的进程间通信的工作方式在 20 世纪 60 年代的操作系统研究中就已经涉及。最早的大范围的分布式系统是局域网，例如以台网。

另外，随着计算机科学的发展和广泛应用，计算的资源和对算力的需求不断增加，单个的计算设备无法存储所有的计算资源或提供需要的所有算力支持，需要多个计算设备协同工作才可以完成任务。

### 3.3 架构

分布式系统有多种架构，比如:

* C/S 架构：多个客户端从服务器处请求数据并格式化显示给用户。在客户端提交的信息可以返回给服务器，来产生一个永久的修改。
* 三层架构：表现层，业务逻辑层，数据访问层。用户的请求在表现层提交，随后传送到业务逻辑层，在业务逻辑层进行验证，计算等工作后提交给数据访问层。数据访问层主要进行数据库的读写操作。
* P2P：没有独立提供服务或管理网络的特殊服务器。所有的任务都被统一在网络之中进行分配。每一个节点的低位都是平等的，并且都可以作为服务器和客户端，例如比特币网络就是通过 P2P 的分布式系统实现的。

### 3.4 分布式系统的优点和使用的必然性

* 增强算力，解决了可扩展问题。对于集中式系统，由于只有一台计算设备，所以在 CPU 和存储设备的单位算力/存储能力一定的情况下，只能通过在这台设备上堆叠 CPU 和存储器的方式提高数据的处理能力，这种方式称为垂直扩展(scale up)。但是这样的提升方式面临许多瓶颈，例如空间的限制，难以满足散热的需求，成本过高，还可能在硬件层面导致计算设备不可用(例如访存时间的增长可能导致在原来的 CPU 架构不能胜任工作)，因此提升的空间是极其有限的。
  如果采用分布式系统，将多台服务器通过网络连接在一起，称为水平扩展(scale out)。即使单台服务器的计算能力和存储能力很有限，整个分布式网络的计算能力也会随着网络中设备数量的增加而变得很可观。
  <img src="img/2.png">
* 提升系统的稳定性，解决了高可用问题。单台服务器面临着损坏的风险，即使没有意外情况发生，服务器也要进行定期的维护和系统升级。这时集中式系统就无法向客户进行服务；而分布式系统即使个别设备出现异常或停机，对整个分布式系统的影响也可能是很小的，通过合理的设计，我们可以让分布式系统的可用性接近 100%。在任何时段，都可以很好地处理用户的请求。
* 降低了数据的延迟。如果只有一台服务器，那么在距离这台服务器较远的位置请求服务数据可能需要较长的延迟。通过分布式系统的设计，我们可以在全球的不同位置部署服务器，用户在有数据访问的需求时只需访问离自己的设备距离最近的服务器即可，这样的设计可以极大地减小集中式系统的一体化带来的数据传输延迟。

### 3.5 当今分布式系统的不足

虽然分布式系统相对于集中式系统有许多优点，但是目前的分布式系统仍然存在许多问题。

* **数据的一致性** 考虑到大量的机器故障：宕机、重启、关机，数据可能丢失、陈旧、出错，如何让系统容纳这些问题，对外保证数据的正确性，需要相当复杂的设计。
* **网络和通信故障** 网络的不可靠，消息可能丢失、早到、迟到、Hang住，这给机器间的协调带来了极大的复杂度。像TCP等网络基础协议，能解决部分问题，但更多的需要系统层面自己处理。更不用说，开放式网络上可能存在的消息伪造。
* **管理复杂度** 机器数量到达一定数量级时，如何对他们进行有效监控、收集日志、负载均衡，都是很大挑战。
* **延迟** 网络通信延迟要比机器内通信高出几个数量级，而组件越多、网络跳数越多，延迟便会更高，这些最终都会作用于系统对外服务质量上。

## 4 ROS 2

ROS(Robot Operating Systems)是一个机器人领域的元操作系统，是一个分布式的通信框架，能够帮助进程间更方便地实现通信，协调一个机器人的各个部件或多个机器人组成的机器人集群完成任务。

### 4.1 ROS vs ROS 2

为了达到简化机器人开发的设计目的，ROS 开发了一整天通讯机制，包括话题，服务参数和动作。为了将各个机器人节点连接起来并实现通信，ROS 引入了 `ROS Master`节点，所有节点的通讯必须经过这个主节点。

<img src="img\3.jpg">

一旦`Ros Master`主节点挂掉后，就会**造成整个系统通信的异常**

ROS 存在以下缺点：

- 通信基于TCP实现，实时性差、系统开销大
- 对Python3支持不友好，需要重新编译
- 消息机制不兼容
- 没有加密机制、安全性不高

在 ROS 2 中，主节点被去掉了，ROS 和 经过改进的 ROS 2 的架构如图所示：

<img src="img\4.png">

ROS 2 相对于 ROS 的改进有：

* 去中心化：ROS 2 取消了 master 节点。去掉 master 节点之后，各个 ROS 2 节点都是平等的，可以实现一对一，一对多，多对多通信。
* 通信使用 DDS 实现，使得 ROS2 的实时性，可靠性和连续性都有了增强。
* 实现了 python2 到 python3 的升级，编译系统由 catkin 换为 ament，C++ 标准更新至 C++11，进程内通信和进程间通信可以使用相同的 API。

### 4.2 ROS 2 的节点之间的通信

<img src="img/5.gif">

## 5 Ray

### 5.1 Ray 简介

Ray是一种通用的集群计算框架，既支持模型的训练，又支持对环境的仿真或与环境的交互，还支持模型服务。Ray所面临的任务涵盖了从轻量级、无状态的计算任务（例如仿真）到长时间运行的、有状态的计算任务（例如训练）。为了满足这些任务的需求，Ray实现了一套统一的接口，这套接口既能表达**基于任务的并行计算**(task-parallel)，又能表达**基于行动器的并行计算**(actor-based)。为了满足性能需求，Ray使用了分布式的任务调度器和元数据存储器设计，从而满足了Ray的毫秒百万级并发量的需求。Ray还为任务和行动器提供了基于Lineage的容错机制，同时为元数据存储提供了基于复制的容错机制。

Ray设计并实现了首个将训练、仿真和服务统一起来的分布式计算框架，基于动态任务执行引擎统一了有状态并行（**行动器**）和无状态并行（**任务**），并保障了框架的高可扩展性和高容错性。

### 5.2 Ray 架构

Ray的架构由应用层和系统层组成，其中应用层实现了Ray的API，作为前端供用户使用，而系统层则作为后端来保障Ray的高可扩展性和容错性。整体的架构图如下图所示：

![](https://pic1.zhimg.com/80/v2-063d38543cd63364eec5994946620b38_720w.jpg)

<center>Ray的架构图


**应用层**

应用层中有三种类型的进程：

- **驱动器进程** (Driver Process): 执行用户程序的进程。顾名思义，所有操作都需要由主进程来驱动。
- **工作器进程** (Worker Process): 执行由驱动器或其他工作器调用的任务（远程函数）的无状态的进程。工作器由系统层分配任务并自动启动。当声明一个远程函数时，该函数将被自动发送到所有的工作器中。在同一个工作器中，任务是串行地执行的，工作器并不维护其任务与任务之间的局部状态，即在工作器中，一个远程函数执行完后，其局部作用域的所有变量将不再能被其他任务所访问。
- **行动器进程** (Actor Process): 行动器被调用时只执行其所暴露的方法。行动器由工作器或驱动器显式地进行实例化。与工作器相同的是，行动器也会串行地执行任务，不同的是行动器上执行的每个方法都依赖于其前面所执行的方法所导致的状态。

三种进程体现到Python代码中如下：

```python
@ray.remote
def f(x):
    # ==== 工作器进程 ====
    return x * x

@ray.remote
class Counter(object):
    def __init__(self):
        # ==== 行动器进程 ====
        self.value = 0

    def increment(self):
        # ==== 行动器进程 ====
        self.value += 1
        return self.value

if __name__ == "__main__":
    # ==== 驱动器进程 ====
    object_ref = f.remote(2)
    assert ray.get(object_ref) == 4

    counter = Counter.remote()
    refs = []
    for i in range(10):
        ref = counter.increment.remote()
        refs.append(ref)
    for i, ref in enumerate(refs):
        assert ray.get(ref) == i + 1
```

**系统层**

系统层由三个主要部件组成：全局控制存储器 (**G**lobal **C**ontrol **S**tore)、分布式调度器 (Distributed Scheduler)和分布式对象存储器 (Distributed Object Store)。这些部件在横向上是可扩展的，即可以增减这些部件的数量，同时还具有一定的容错性。

**GCS**

GCS设计的初衷是让系统中的各个组件都变得尽可能地无状态，因此GCS维护了一些全局状态：

- 对象表 (Object Table)：记录每个对象存在于哪些节点
- 任务表 (Task Table)：记录每个任务运行于哪个节点
- 函数表 (Function Table)：记录用户进程中定义的远程函数
- 事件日志 (Event Logs)：记录任务运行日志

**分布式调度器**

Ray中的任务调度器被分为两层，由一个全局调度器和每个节点各自的局部调度器组成。为了避免全局调度器负载过重，**在节点创建的任务首先被提交到局部调度器，如果该节点没有过载且节点资源能够满足任务的需求（如GPU的需求），则任务将在本地被调度，否则任务才会被传递到全局调度器**，考虑将任务调度到远端。由于Ray首先考虑在本地调度，本地不满足要求才考虑在远端调用，因此这样的调度方式也被称为自底向上的调度。

下图展示了Ray的调度过程，箭头的粗细表示过程发生频率的高低。用户进程和工作器向本地调度器提交任务，大多数情况下，任务将在本地被调度。少数情况下，局部调度器会向全局调度器提交任务，并向GCS传递任务的相关信息，将任务涉及的对象和函数存入全局的对象表和函数表中，然后全局调度器会从GCS中读取到信息，并选择在其他合适的节点上调度这一任务。更具体地来说，全局调度器会根据任务的请求选出具有足够资源的一系列节点，并在这些节点中选出等待时间最短的一个节点。

![](https://pic1.zhimg.com/80/v2-b047e880cf58ec9c6670778b84fd5910_720w.jpg)

**分布式对象存储器**

Ray实现了一个内存式的分布式存储系统来存储每个任务的输入和输出。Ray通过内存共享机制在每个节点上实现了一个对象存储器 (Object Store)，从而使在同一个节点运行的任务之间不需要拷贝就可以共享数据。当一个任务的输入不在本地时，则会在执行之前将它的输入复制到本地的对象存储器中。同样地，任务总会将输出写入到本地的对象存储器中。这样的复制机制可以减少任务的执行时间，因为**任务永远只会从本地对象存储器中读取数据（否则任务不会被调度）**，并且消除了热数据可能带来的潜在的瓶颈。

### 5.3 Ray 编程

Ray中有两个重要的概念：**任务**(Task)和**行动器**(Actor)。**Ray编程模型**是指Ray框架基于任务和行动器这两个重要需求所向用户提供的一套API及其编程范式。下表展示了Ray提供的核心API。

| 代码                                                         | 说明                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| futures = f.remote(args)                                     | 远程地执行函数f。f.remote()以普通对象或future对象作为输入，返回一个或多个future对象，非阻塞执行。 |
| objects = ray.get(futures)                                   | 返回与一个或多个future对象相关联的真实值，阻塞执行           |
| ready_futures = ray.wait(futures, k, timeout)                | 当futures中有k个future完成时，或执行时间超过timeout时，返回futures中已经执行完的future |
| actor = Class.remote(args) futures = actor.method.remote(args) | 将一个类实例化为一个远程的行动器，并返回它的一个句柄。然后调用这个行动器的method方法，并返回一个或多个future. 两个过程均为非阻塞的。 |

任务是指在无状态的工作器中执行的远程函数。远程函数被调用时会立即返回一个future对象，而真正的返回值可以通过ray.get(<future对象>)的方式来获取。**这样的编程模型既允许用户编写并行计算代码，同时又提醒用户要关注数据之间的依赖性。**

**任务的编程范式**如下：

1. 注册任务：在需要注册为任务的函数上加上@ray.remote装饰器
2. 提交任务：在调用具有@ray.remote装饰器的函数时，需要带上.remote()而不是直接调用
3. 非阻塞提交：无论任务的运行需要多少时间，在提交任务后都会立即返回一个ObjectRef对象
4. 按需阻塞获取结果：在你需要函数的返回值时，可以通过ray.get来获取

**行动器的编程范式**如下：

1. 注册行动器：在需要注册为行动器的类上加上@ray.remote装饰器
2. 实例化行动器：相比于普通Python类的实例化，需要在类名后加上.remote
3. 提交方法调用：调用行动器的方法时，同样需要带上.remote()而不是直接调用
4. 非阻塞提交：无论方法的运行需要多少时间，在提交任务后都会立即返回一个ObjectRef对象（同一行动器实例下，方法会按照提交顺序串行地运行）
5. 按需阻塞获取结果：在需要方法运行的返回值时，可以通过ray.get来获取

## 6 项目意义

通过我们的工作，ROS 2 各个节点在保证运算正确性的前提下提高了并行度，提升了计算过程中 CPU 和内存等资源的利用效率，增强了可用性。我们使用 ROS 2 实现相互之间可以通过话题，行为等方式相互通信的节点，并使用 Ray 实现计算任务在各个节点之间的分配，并对一些测试用例在不同情况下的执行情况进行了测试。
